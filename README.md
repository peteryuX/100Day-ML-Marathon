# 100Day-ML-Marathon

![Alt text](./photos/cover.JPG)
Personal upload space for the [100Day-ML-Marathon](https://ai100-2.cupoy.com), which is a marathon of training your machine learning skill on [Kaggle](https://www.kaggle.com) in 100 consecutive days.

### Daily Topic
#### Clean Data and Data Preprocessing
- Day 01 : Data Introduction and Assessment
- Day 02 : Exploratory Data Analysis(EDA)
- Day 03 : Build [Pandas](https://pandas.pydata.org/) DataFrame
- Day 04 : Pandas Data Types
- Day 05 : EDA Distribution
- Day 06 : Handle Outlier Data
- Day 07 : Normalize Continuous Data
- Day 08 : DataFrame operation / Data frame merge
- Day 09 : EDA Correlation 1
- Day 10 : EDA Correlation 2
- Day 11 : Kernal Density Estimation (KDE)
- Day 12 : Discretization Method
- Day 13 : Implement Discretization Method
- Day 14 : Subplot using [Matplotib](https://matplotlib.org/)
- Day 15 : Heatmap and Grid-plot
- Day 16 : Logistic Regression
### Feature Engineering
- Day 17 : Introduction of Feature Engineering
- Day 18 : Feture Types
- Day 19 : [Value Type] Insert Value for Lost Information
- Day 20 : [Value Type] Remove Outlier
- Day 21 : [Value Type] Remove Bias
- Day 22 : [Class Type] One-Hot and Label Encoding
- Day 23 : [Class Type] Mean Encoding
- Day 24 : [Class Type] Other Advanced Processing
- Day 25 : [Time Type] Time Cycle
- Day 26 : Feature Combination (Value and Value)
- Day 27 : Feature Combination (Value and Class)
- Day 28 : Feature Selection
- Day 29 : Feature Estimation
- Day 30 : Leaf Encoding on Class Type Feature
### Machine Learning Model Building
- Day 31 : Introduction of Machine Learning
- Day 32 : Framework and Process in Machine Learning
- Day 33 : How to Teach Machine?
- Day 34 : Split Training and Evaluation Set
- Day 35 : Regression vs. Classification
- Day 36 : Evaluation Metrics
- Day 37 : Regression Model Introdoction ([Linear](https://en.wikipedia.org/wiki/Linear_regression) / [Logistic](https://en.wikipedia.org/wiki/Logistic_regression))
- Day 38 : Rgression Model Implement (Linear / Logistic)
- Day 39 : Regression Model Introdoction ([LASSO](https://en.wikipedia.org/wiki/Lasso_(statistics)) / [Ridge](https://en.wikipedia.org/wiki/Tikhonov_regularization))
- Day 40 : Rgression Model Implement (LASSO / Ridge)
- Day 41 : Tree Based Model Introdoction ([Decision Tree](https://en.wikipedia.org/wiki/Decision_tree))
- Day 42 : Tree Based Model Implement (Decision Tree)
- Day 43 : Tree Based Model Introdoction ([Random Forest](https://en.wikipedia.org/wiki/Random_forest))
- Day 44 : Tree Based Model Implement (Random Forest)
- Day 45 : Tree Based Model Introdoction ([Gradient Boosting Machine](https://en.wikipedia.org/wiki/Gradient_boosting))
- Day 46 : Tree Based Model Implement (Gradient Boosting Machine)
### Machine Learning Fine-tuning
- Day 47 : Hyper-Parameters Tuning and Optimization
- Day 48 : Introduction of [Kaggle](https://www.kaggle.com)
- Day 49 : Bleding Method
- Day 50 : Stacking Method
### Mid-Term Exam
- Day 51 : Mid-Term Exam (1/3)
- Day 52 : Mid-Term Exam (2/3)
- Day 53 : Mid-Term Exam (3/3)
### Unsupervised Learning
- Day 54 : Introduction of Unsupervised Learning
- Day 55 : Clustering Method
- Day 56 : [K-Mean](https://en.wikipedia.org/wiki/K-means_clustering)
- Day 57 : Hierarchical Clustering
- Day 58 : Hierarchical Clustering on 2D Toy Dataset
- Day 59 : Dimension Reduction - [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis)
- Day 60 : PCA on [MNIST](http://yann.lecun.com/exdb/mnist/)
- Day 61 : Dimension Reduction - [T-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding)
- Day 62 : T-SNE Implement
### Deep Learning Theory and Implement
- Day 63 : Introduction of Neural Netork
- Day 64 : Experience on [TensorFlow PlayGround](https://playground.tensorflow.org) (Learning Rate)
- Day 65 : Experience on [TensorFlow PlayGround](https://playground.tensorflow.org) (Activation Function/ Regularization)
### Deep Learning on [Keras](https://keras.io/)
- Day 66 : Introducion of Keras
- Day 67 : Keras Dataset
- Day 68 : Keras Sequential API
- Day 69 : Keras Module API
- Day 70 : Multi-Layer Perception (MLP)
- Day 71 : Loss Functions
- Day 72 : Activation Function
- Day 73 : Gradient Descend (1/2)
- Day 74 : Gradient Descend (2/2)
- Day 75 : Back Propagation
- Day 76 : Optimizers
- Day 77 : Validation and Overfitting
- Day 78 : KeyNote before Training Model
- Day 79 : Learning Rate Effect
- Day 80 : Combination of Optomizer and Learning Rate
- Day 81 : Avoid Overfitting - Regularization
- Day 82 : Avoid Overfitting - [Dropout](https://en.wikipedia.org/wiki/Dropout_(neural_networks))
- Day 83 : Avoid Overfitting - [Batch Normalization](https://en.wikipedia.org/wiki/Batch_normalization)
- Day 84 : Avoid Overfitting - Hyper-Parameters Tuning and Comparison
- Day 85 : Avoid Overfitting - Early Stop
- Day 86 : Saving and Restoring Model
- Day 87 : Learning Rate Decay
- Day 88 : Design your Keras Callbacks Function
- Day 89 : Design your Loss Funciton
- Day 90 : Image Recognition using Tranditional Computer Vsion Methods 
- Day 91 : Image Recognition using Machine Learning Model
### [Convolutional Neural Network (CNN)](https://en.wikipedia.org/wiki/Convolutional_neural_network) in Deep Learning
- Day 92 : Introdoction of CNN (1/2)
- Day 93 : Introdoction of CNN (2/2)
- Day 94 : Parameters Tuning in CNN Layer
- Day 95 : Pooling Layer in Keras
- Day 96 : CNN Layer in Keras
- Day 97 : CNN vs. DNN on [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html)
- Day 98 : Data Generator in Keras
- Day 99 : Data Augmentation in Keras
- Day 100 : Transfer Learning
### Final Exam
- Day 101 : Final Exam (1/3)
- Day 102 : Final Exam (2/3)
- Day 103 : Final Exam (3/3)